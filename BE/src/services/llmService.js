import OpenAI from 'openai';
import axios from 'axios'; 

// --- C·∫§U H√åNH ---
// 1. Link AI c·ªßa b·∫°n (Kaggle/Ngrok)
const LOCAL_API_URL = 'https://lakier-jewell-nonhygroscopically.ngrok-free.dev/analyze'; // Thay b·∫±ng link c·ªßa b·∫°n

// 2. C·∫•u h√¨nh OpenAI (M·∫∑c ƒë·ªãnh cho Chatbot)
const DEFAULT_PROVIDER = process.env.LLM_PROVIDER || 'openai'; 
const DEFAULT_OPENAI_MODEL = process.env.OPENAI_MODEL || 'gpt-4o-mini';

let openaiClient = null;
if (process.env.OPENAI_API_KEY) {
  openaiClient = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
} else {
  console.warn('[LLM] Thi·∫øu OPENAI_API_KEY. Chatbot s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.');
}

const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));

/**
 * H√†m Chat ƒêa NƒÉng
 * - Chatbot th∆∞·ªùng: Kh√¥ng truy·ªÅn g√¨ -> T·ª± d√πng OpenAI.
 * - Gi·∫£i m√£ gi·∫•c m∆°: Truy·ªÅn { provider: 'local' } -> D√πng model c·ªßa b·∫°n.
 */
export async function chat(messages, opts = {}) {
  // M·∫∑c ƒë·ªãnh d√πng OpenAI, tr·ª´ khi ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh 'local'
  const provider = opts.provider || 'openai';

  // === TR∆Ø·ªúNG H·ª¢P 1: D√ôNG MODEL C·ª¶A B·∫†N (Kaggle) ===
  if (provider === 'local') {
    try {
      const userMessage = messages.find(m => m.role === 'user')?.content || '';
      if (!userMessage) throw new Error("N·ªôi dung tin nh·∫Øn tr·ªëng");

      console.log(`[LLM-Local] üîÆ ƒêang g·ª≠i sang Kaggle: "${userMessage.substring(0, 30)}..."`);
      
      const response = await axios.post(LOCAL_API_URL, {
        dream: userMessage
      });

      return { text: response.data.result || '' };
    } catch (e) {
      console.error('‚ùå [LLM-Local] L·ªói k·∫øt n·ªëi Kaggle:', e.message);
      throw new Error('AI Server (Kaggle) ƒëang t·∫Øt ho·∫∑c b·ªã l·ªói k·∫øt n·ªëi.');
    }
  }

  // === TR∆Ø·ªúNG H·ª¢P 2: D√ôNG OPENAI (Cho Chatbot) ===
  if (provider === 'openai') {
    if (!openaiClient) throw new Error('Ch∆∞a c·∫•u h√¨nh OpenAI API Key.');

    const model = opts.model || DEFAULT_OPENAI_MODEL;
    const maxRetries = opts.maxRetries || 2;
    
    for (let attempt = 0; attempt <= maxRetries; attempt++) {
      try {
        const res = await openaiClient.chat.completions.create({
          model,
          messages,
          temperature: opts.temperature ?? 0.7,
          max_tokens: opts.max_tokens ?? 500,
        });
        const text = res?.choices?.[0]?.message?.content?.trim() || '';
        return { text, raw: res };
      } catch (e) {
        if (e?.status === 429 && attempt < maxRetries) {
          await sleep(1000 * Math.pow(2, attempt));
          continue;
        }
        throw e;
      }
    }
  }
}

// H√†m n√†y gi·ªØ nguy√™n d√πng OpenAI ƒë·ªÉ x·ª≠ l√Ω JSON cho chu·∫©n
export async function structuredJSON(prompt, schemaHint, opts = {}) {
  const sys = 'You are a JSON-only assistant. Always reply with valid JSON only.';
  const messages = [
    { role: 'system', content: sys },
    { role: 'user', content: `${prompt}\n\nReturn JSON with this shape: ${schemaHint}` },
  ];
  // Lu√¥n √©p d√πng OpenAI cho h√†m n√†y
  const { text } = await chat(messages, { ...opts, provider: 'openai', temperature: 0.2 });
  try {
    const cleaned = text.replace(/^```json\n?|```$/g, '').trim();
    return JSON.parse(cleaned);
  } catch (e) {
    return { _raw: text, error: 'JSON_PARSE_ERROR' };
  }
}

export default { chat, structuredJSON };