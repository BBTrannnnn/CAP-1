import os
import time

# ==============================================================================
# 0. C√ÄI ƒê·∫∂T M√îI TR∆Ø·ªúNG (CH·∫†Y 1 L·∫¶N)
# ==============================================================================
print("‚è≥ ƒêang thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng s·∫°ch...")

# C√†i ƒë·∫∑t b·ªô t·ª© quy·ªÅn l·ª±c cho Fine-tuning 4-bit
# -U: Upgrade l√™n b·∫£n m·ªõi nh·∫•t ·ªïn ƒë·ªãnh
# bitsandbytes: Qu·∫£n l√Ω b·ªô nh·ªõ GPU
# peft: K·ªπ thu·∫≠t LoRA
# accelerate: TƒÉng t·ªëc training
# transformers: Th∆∞ vi·ªán l√µi
os.system("pip install -q -U bitsandbytes peft accelerate datasets trl")
os.system("pip install -q -U transformers") 

print("‚úÖ ƒê√£ c√†i xong th∆∞ vi·ªán! ƒêang kh·ªüi ƒë·ªông Training...")

# ==============================================================================
# 1. IMPORT & C·∫§U H√åNH
# ==============================================================================
import torch
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# Model Vinallama (B·∫£n 7B Chat)
MODEL_NAME = "vilm/vinallama-7b-chat"
NEW_MODEL_NAME = "vinallama-dream-interpreter"

# üëá ƒê∆∞·ªùng d·∫´n file d·ªØ li·ªáu c·ªßa b·∫°n (Ki·ªÉm tra k·ªπ t√™n folder)
DATA_PATH = "/kaggle/input/dreamed/dream_data_vietnamese_v2.csv"

# ==============================================================================
# 2. X·ª¨ L√ù D·ªÆ LI·ªÜU
# ==============================================================================
print(f"üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: {DATA_PATH}")

try:
    df = pd.read_csv(DATA_PATH)
    # Chu·∫©n h√≥a t√™n c·ªôt (v·ªÅ ch·ªØ th∆∞·ªùng, x√≥a kho·∫£ng tr·∫Øng)
    df.columns = [c.lower().strip() for c in df.columns]
    
    # Map t√™n c·ªôt n·∫øu n√≥ kh√¥ng ƒë√∫ng chu·∫©n
    column_mapping = {
        'content': 'dream', 'gi·∫•c m∆°': 'dream',
        'meaning': 'output', 'gi·∫£i m√£': 'output', '√Ω nghƒ©a': 'output'
    }
    df.rename(columns=column_mapping, inplace=True)
    
    # Ki·ªÉm tra l·∫ßn cu·ªëi
    if 'dream' not in df.columns or 'output' not in df.columns:
        raise ValueError(f"File thi·∫øu c·ªôt 'dream' ho·∫∑c 'output'. C√°c c·ªôt hi·ªán c√≥: {list(df.columns)}")
    
    dataset = Dataset.from_pandas(df[['dream', 'output']])
    print(f"‚úÖ D·ªØ li·ªáu OK: {len(dataset)} d√≤ng.")

except Exception as e:
    print(f"‚ùå L·ªói d·ªØ li·ªáu: {e}")
    exit()

# ==============================================================================
# 3. T·∫¢I MODEL & TOKENIZER
# ==============================================================================
print("‚è≥ ƒêang t·∫£i Vinallama (S·∫Ω m·∫•t kho·∫£ng 2-3 ph√∫t)...")

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # C·∫•u h√¨nh 4-bit (Ti·∫øt ki·ªám VRAM)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=False,
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    
    # Chu·∫©n b·ªã model (T·∫Øt cache ƒë·ªÉ ti·∫øt ki·ªám RAM khi train)
    model.config.use_cache = False 
    model.config.pretraining_tp = 1
    model = prepare_model_for_kbit_training(model)
    
except Exception as e:
    print(f"‚ùå L·ªói t·∫£i Model: {e}")
    exit()

# ==============================================================================
# 4. G·∫ÆN LORA ADAPTER
# ==============================================================================
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ==============================================================================
# 5. FORMAT D·ªÆ LI·ªÜU (CHATML CHO VINALLAMA)
# ==============================================================================
def format_chatml(example):
    # Prompt √©p khu√¥n chuy√™n gia
    text = f"""<|im_start|>system
B·∫°n l√† m·ªôt chuy√™n gia t√¢m l√Ω v√† phong th·ªßy uy t√≠n. H√£y gi·∫£i m√£ gi·∫•c m∆° sau ƒë√¢y m·ªôt c√°ch chi ti·∫øt, nh√¢n vƒÉn v√† ƒë∆∞a ra l·ªùi khuy√™n h·ªØu √≠ch.
<|im_end|>
<|im_start|>user
{example['dream']}
<|im_end|>
<|im_start|>assistant
{example['output']}
<|im_end|>"""
    
    # Tokenize
    tokenized = tokenizer(
        text,
        truncation=True,
        max_length=512,
        padding="max_length"
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

print("üõ† ƒêang x·ª≠ l√Ω d·ªØ li·ªáu...")
tokenized_dataset = dataset.map(format_chatml, remove_columns=dataset.column_names)

# ==============================================================================
# 6. B·∫ÆT ƒê·∫¶U TRAIN
# ==============================================================================
training_args = TrainingArguments(
    output_dir="./results_vinallama",
    num_train_epochs=1,              # 1 Epoch l√† ƒë·ªß v·ªõi 1000 d√≤ng
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    optim="paged_adamw_32bit",
    save_steps=50,
    logging_steps=10,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=True,
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="cosine",
    report_to="none"
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_dataset,
    args=training_args,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

print("üöÄ START TRAINING...")
trainer.train()

# ==============================================================================
# 7. L∆ØU TH√ÄNH QU·∫¢
# ==============================================================================
save_path = f"/kaggle/working/{NEW_MODEL_NAME}"
print(f"üíæ ƒêang l∆∞u model t·∫°i: {save_path}")

trainer.model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print("‚úÖ TRAINING TH√ÄNH C√îNG! H√£y t·∫£i folder v·ªÅ.")