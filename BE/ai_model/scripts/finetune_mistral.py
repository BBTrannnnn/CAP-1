import os
import torch
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

#Cáº¤U HÃŒNH VÃ€ Táº¢I Dá»® LIá»†U
MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.2"
NEW_MODEL_NAME = "mistral-dream-interpreter"

DATA_PATH = "/kaggle/input/deamclean/dreams_clean.csv" 

#Kiá»ƒm tra xem file cÃ³ tá»“n táº¡i khÃ´ng
print(f"Äang Ä‘á»c dá»¯ liá»‡u tá»«: {DATA_PATH}")
df = pd.read_csv(DATA_PATH)

#Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u sang Ä‘á»‹nh dáº¡ng Dataset cá»§a HuggingFace
dataset = Dataset.from_pandas(df[['dream', 'output']])

#Táº¢I TOKENIZER VÃ€ MODEL (QLoRA 4-bit)
print("Äang táº£i Tokenizer vÃ  Model...")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
#CÃ i Ä‘áº·t padding token (Mistral máº·c Ä‘á»‹nh khÃ´ng cÃ³)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" #trÃ¡nh lá»—i khi train fp16

#Cáº¥u hÃ¬nh lÆ°á»£ng tá»­ hÃ³a 4-bit Ä‘á»ƒ cháº¡y Ä‘Æ°á»£c trÃªn GPU T4 cá»§a Kaggle
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

#Chuáº©n bá»‹ model Ä‘á»ƒ training 4-bit
model.config.use_cache = False 
model.config.pretraining_tp = 1
model = prepare_model_for_kbit_training(model)

#Cáº¤U HÃŒNH LORA (FINE-TUNING EFFICIENT) ---
peft_config = LoraConfig(
    r=16,                    # TÄƒng rank lÃªn 16 Ä‘á»ƒ model há»c tá»‘t hÆ¡n (máº·c Ä‘á»‹nh 8)
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    # Target vÃ o táº¥t cáº£ cÃ¡c module linear Ä‘á»ƒ káº¿t quáº£ tá»‘t nháº¥t
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters() # In ra sá»‘ lÆ°á»£ng tham sá»‘ sáº½ train

#Xá»¬ LÃ Dá»® LIá»†U (PROMPT TEMPLATE) 
#Format chuáº©n cho Mistral Instruct: <s>[INST] Instruction [/INST] Model answer</s>
def preprocess(example):
    # Táº¡o prompt hÆ°á»›ng dáº«n
    prompt = f"<s>[INST] Báº¡n lÃ  chuyÃªn gia giáº£i mÃ£ giáº¥c mÆ¡. HÃ£y phÃ¢n tÃ­ch giáº¥c mÆ¡ sau vÃ  Ä‘Æ°a ra lá»i khuyÃªn: {example['dream']} [/INST] {example['output']} </s>"
    
    # Tokenize
    tokenized = tokenizer(
        prompt,
        truncation=True,
        max_length=512, # Äá»™ dÃ i tá»‘i Ä‘a context
        padding="max_length"
    )
    
    # Vá»›i Causal LM, labels chÃ­nh lÃ  input_ids
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

print("ğŸ›  Äang xá»­ lÃ½ dá»¯ liá»‡u...")
tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)

#THIáº¾T Láº¬P TRAINING
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=5,              # Sá»‘ vÃ²ng láº·p train (tÄƒng lÃªn 3-5 náº¿u muá»‘n káº¿t quáº£ tá»‘t hÆ¡n)
    per_device_train_batch_size=4,   # TÄƒng lÃªn náº¿u GPU cÃ²n trá»‘ng RAM
    gradient_accumulation_steps=1,
    optim="paged_adamw_32bit",       # Optimizer tá»‘i Æ°u bá»™ nhá»›
    save_steps=50,
    logging_steps=10,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=True,                       # Sá»­ dá»¥ng mixed precision
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant",
    report_to="none"                 # Táº¯t report wandb cho Ä‘Æ¡n giáº£n
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_dataset,
    args=training_args,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

#Báº®T Äáº¦U TRAIN
print("Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh Fine-tuning...")
trainer.train()

#LÆ¯U MODEL
save_path = f"/kaggle/working/{NEW_MODEL_NAME}"
print(f"Äang lÆ°u model táº¡i: {save_path}")

trainer.model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print("Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u")